broadcasting
C = A + b
- add vect b to each row of matrix a
hadamard product
- element-wise multiplication of matrices
Inverse Matrices
- only exist if matrix is one-to-one
-- no/infinite solutions possible => no inverse
Column Space (range)
- linear combination of matrix columns
-- n >= m (# cols >= # rows): solution possible
--- redundant columns negate extra columns (same cols 2x2 -> 2x1)
-- n == m: unique solution
--- if columns are linearly independent
linear independence
- no vector in set can be expressed as linear combo of other vectors in set
-- linear combo: sum of unique scalar mult each thing
- for a matrix to span R^n, req n linearly independent columns
non-singular
- linearly independent columns
* norms
- L^2: euclidean norm
- squared L^2: x^T x (easier to work w/ in many cases)
- L^1 norm: sum the abs val of all elems
- L^infinity: simplifies to single max element
matrix norm
- Frobenius norm: sum square of all elems in matrix
-- matrix analog to L^2 norm
* special matrices and vectors
** diagonal
element-wise scaling
- all x's scaled by x term, all y's scaled by y term, etc
inverse exists iff all diags non-zero
diag^-1 = 1/x_i for all i
non-square diag
- tall: concat zero rows onto resultant matrix
- wide: strip final elems of each vector multiplied
** symmetric
A = A^T
often results from fn of two args, if not dependent on order
** unit vector
norm = 1
** orthogonal vectors
x^T y = 0
** orthonormal
orthogonal unit vectors
** orthogonal matrix (all rows and columns *orthonormal*)
rows orthonormal
columns orthonormal
A^T A = A A^T = I
A-1 = A^T
* Eigendecomposition
decompose matric transformation into functional properties
- Av = lambda v
-- lambda is a scalar
find vector(s) that are only scaled by the matrix
- scalar multiples of eigenvectors are thusly scaled as well
-- so just find unit eigenvectors
eigendecomposition
- A = V Lambda V^-1
-- derivation
A V => V Lambda
A = V Lambda V^-1
* Singular Value Decomposition
A = U D V^T
A: MxN
U: MxM => eigenvectors of A A^T (?)
D: MxN
V: NxN => eigenvectors of A^T A
* Moore-Penrose Pseudoinverse (I think this is used in my other notes...look it up)
pseudo-generalization of inverse to non-square matrices
A^+ = lim(a->0) (A^T A + aI)^-1 A^T
for practicality, use SVD
A+ ~= V D^+ U^T
D^+: take reciprocal of diag elems, then transpose
solving: Ax = y => x = A^+ y
- yields one of many possible solutions
-- the one with minimal norm
* Trace
sum of diag elems
Froebenius norm = sqrt[ Tr(A^T A) ]
trace cycling: move last to front
Tr(ABC) = Tr(CAB) = Tr(BCA)
- works for matrices of any dimension
? don't really see that much utility here, but whatevs
* Determinant
*product of all eigenvalues*
change in length/area/volume/hyper...
* Principal Component Analysis
can be derived from stuff above
