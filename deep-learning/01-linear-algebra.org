broadcasting
C = A + b
- add vect b to each row of matrix a
hadamard product
- element-wise multiplication of matrices
Inverse Matrices
- only exist if matrix is one-to-one
-- no/infinite solutions possible => no inverse
Column Space (range)
- linear combination of matrix columns
-- n >= m (# cols >= # rows): solution possible
--- redundant columns negate extra columns (same cols 2x2 -> 2x1)
-- n == m: unique solution
--- if columns are linearly independent
linear independence
- no vector in set can be expressed as linear combo of other vectors in set
-- linear combo: sum of unique scalar mult each thing
- for a matrix to span R^n, req n linearly independent columns
non-singular
- linearly independent columns
* norms
- L^2: euclidean norm
- squared L^2: x^T x (easier to work w/ in many cases)
- L^1 norm: sum the abs val of all elems
- L^infinity: simplifies to single max element
matrix norm
- Frobenius norm: sum square of all elems in matrix
-- matrix analog to L^2 norm
* special matrices and vectors
** diagonal
element-wise scaling
- all x's scaled by x term, all y's scaled by y term, etc
inverse exists iff all diags non-zero
diag^-1 = 1/x_i for all i
non-square diag
- tall: concat zero rows onto resultant matrix
- wide: strip final elems of each vector multiplied
** symmetric
A = A^T
often results from fn of two args, if not dependent on order
** unit vector
norm = 1
** orthogonal vectors
x^T y = 0
** orthonormal
orthogonal unit vectors
** orthogonal matrix (all rows and columns *orthonormal*)
rows orthonormal
columns orthonormal
A^T A = A A^T = I
A-1 = A^T
* Eigendecomposition
decompose matric transformation into functional properties
- Av = lambda v
-- lambda is a scalar
find vector(s) that are only scaled by the matrix
- scalar multiples of eigenvectors are thusly scaled as well
-- so just find unit eigenvectors
eigendecomposition
- A = V Lambda V^-1
-- derivation
A V => V Lambda
A = V Lambda V^-1




